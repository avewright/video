{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[27], line 12\u001b[0m\n",
      "\u001b[0;32m      9\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Prepare the image for the model\u001b[39;00m\n",
      "\u001b[1;32m---> 12\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# No text input needed for classification\u001b[39;49;00m\n",
      "\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Add dummy bounding box\u001b[39;49;00m\n",
      "\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n",
      "\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n",
      "\u001b[0;32m     20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoding)\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\layoutlmv3\\processing_layoutlmv3.py:124\u001b[0m, in \u001b[0;36mLayoutLMv3Processor.__call__\u001b[1;34m(self, images, text, text_pair, boxes, word_labels, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, return_tensors, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    120\u001b[0m         text \u001b[38;5;241m=\u001b[39m [text]  \u001b[38;5;66;03m# add batch dimension (as the image processor always adds a batch dimension)\u001b[39;00m\n",
      "\u001b[0;32m    121\u001b[0m     text_pair \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;32m    123\u001b[0m encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n",
      "\u001b[1;32m--> 124\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwords\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n",
      "\u001b[0;32m    125\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[0;32m    126\u001b[0m     boxes\u001b[38;5;241m=\u001b[39mboxes \u001b[38;5;28;01mif\u001b[39;00m boxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n",
      "\u001b[0;32m    127\u001b[0m     word_labels\u001b[38;5;241m=\u001b[39mword_labels,\n",
      "\u001b[0;32m    128\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n",
      "\u001b[0;32m    129\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n",
      "\u001b[0;32m    130\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n",
      "\u001b[0;32m    131\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n",
      "\u001b[0;32m    132\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n",
      "\u001b[0;32m    133\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n",
      "\u001b[0;32m    134\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n",
      "\u001b[0;32m    135\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n",
      "\u001b[0;32m    136\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n",
      "\u001b[0;32m    137\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n",
      "\u001b[0;32m    138\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n",
      "\u001b[0;32m    139\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n",
      "\u001b[0;32m    140\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n",
      "\u001b[0;32m    141\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n",
      "\u001b[0;32m    142\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n",
      "\u001b[0;32m    143\u001b[0m )\n",
      "\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# add pixel values\u001b[39;00m\n",
      "\u001b[0;32m    146\u001b[0m images \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\feature_extraction_utils.py:86\u001b[0m, in \u001b[0;36mBatchFeature.__getitem__\u001b[1;34m(self, item)\u001b[0m\n",
      "\u001b[0;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03mIf the key is a string, returns the value of the dict associated to `key` ('input_values', 'attention_mask',\u001b[39;00m\n",
      "\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03metc.).\u001b[39;00m\n",
      "\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexing with integers is not available when using Python based feature extractors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "\u001b[1;31mKeyError\u001b[0m: 'words'"
     ]
    }
   ],
   "source": [
    "# Create a test image (you would replace this with your actual image)\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Download a sample invoice image for testing\n",
    "# url = \"https://raw.githubusercontent.com/microsoft/unilm/master/layoutlmv3/examples/invoice.png\"\n",
    "# response = requests.get(url)\n",
    "image = Image.open(\"image.png\").convert('RGB')\n",
    "\n",
    "# Prepare the image for the model\n",
    "encoding = processor(\n",
    "    images=image,\n",
    "    text=None,  # No text input needed for classification\n",
    "    boxes=[[0, 0, 1000, 1000]],  # Add dummy bounding box\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Forward pass through the model\n",
    "outputs = model(**encoding)\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = outputs.logits.argmax(-1).item()\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "\n",
    "# Get the confidence scores\n",
    "confidence_scores = outputs.logits.softmax(-1)\n",
    "print(f\"Confidence scores: {confidence_scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[27], line 12\u001b[0m\n",
      "\u001b[0;32m      9\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Prepare the image for the model\u001b[39;00m\n",
      "\u001b[1;32m---> 12\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# No text input needed for classification\u001b[39;49;00m\n",
      "\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Add dummy bounding box\u001b[39;49;00m\n",
      "\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n",
      "\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n",
      "\u001b[0;32m     20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoding)\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\layoutlmv3\\processing_layoutlmv3.py:124\u001b[0m, in \u001b[0;36mLayoutLMv3Processor.__call__\u001b[1;34m(self, images, text, text_pair, boxes, word_labels, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, return_tensors, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    120\u001b[0m         text \u001b[38;5;241m=\u001b[39m [text]  \u001b[38;5;66;03m# add batch dimension (as the image processor always adds a batch dimension)\u001b[39;00m\n",
      "\u001b[0;32m    121\u001b[0m     text_pair \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;32m    123\u001b[0m encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n",
      "\u001b[1;32m--> 124\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwords\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n",
      "\u001b[0;32m    125\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[0;32m    126\u001b[0m     boxes\u001b[38;5;241m=\u001b[39mboxes \u001b[38;5;28;01mif\u001b[39;00m boxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n",
      "\u001b[0;32m    127\u001b[0m     word_labels\u001b[38;5;241m=\u001b[39mword_labels,\n",
      "\u001b[0;32m    128\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n",
      "\u001b[0;32m    129\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n",
      "\u001b[0;32m    130\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n",
      "\u001b[0;32m    131\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n",
      "\u001b[0;32m    132\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n",
      "\u001b[0;32m    133\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n",
      "\u001b[0;32m    134\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n",
      "\u001b[0;32m    135\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n",
      "\u001b[0;32m    136\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n",
      "\u001b[0;32m    137\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n",
      "\u001b[0;32m    138\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n",
      "\u001b[0;32m    139\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n",
      "\u001b[0;32m    140\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n",
      "\u001b[0;32m    141\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n",
      "\u001b[0;32m    142\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n",
      "\u001b[0;32m    143\u001b[0m )\n",
      "\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# add pixel values\u001b[39;00m\n",
      "\u001b[0;32m    146\u001b[0m images \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\feature_extraction_utils.py:86\u001b[0m, in \u001b[0;36mBatchFeature.__getitem__\u001b[1;34m(self, item)\u001b[0m\n",
      "\u001b[0;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03mIf the key is a string, returns the value of the dict associated to `key` ('input_values', 'attention_mask',\u001b[39;00m\n",
      "\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03metc.).\u001b[39;00m\n",
      "\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexing with integers is not available when using Python based feature extractors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "\u001b[1;31mKeyError\u001b[0m: 'words'"
     ]
    }
   ],
   "source": [
    "# Create a test image (you would replace this with your actual image)\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Download a sample invoice image for testing\n",
    "# url = \"https://raw.githubusercontent.com/microsoft/unilm/master/layoutlmv3/examples/invoice.png\"\n",
    "# response = requests.get(url)\n",
    "image = Image.open(\"image.png\").convert('RGB')\n",
    "\n",
    "# Prepare the image for the model\n",
    "encoding = processor(\n",
    "    images=image,\n",
    "    text=None,  # No text input needed for classification\n",
    "    boxes=[[0, 0, 1000, 1000]],  # Add dummy bounding box\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Forward pass through the model\n",
    "outputs = model(**encoding)\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = outputs.logits.argmax(-1).item()\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "\n",
    "# Get the confidence scores\n",
    "confidence_scores = outputs.logits.softmax(-1)\n",
    "print(f\"Confidence scores: {confidence_scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[27], line 12\u001b[0m\n",
      "\u001b[0;32m      9\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Prepare the image for the model\u001b[39;00m\n",
      "\u001b[1;32m---> 12\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# No text input needed for classification\u001b[39;49;00m\n",
      "\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Add dummy bounding box\u001b[39;49;00m\n",
      "\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n",
      "\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n",
      "\u001b[0;32m     20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoding)\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\layoutlmv3\\processing_layoutlmv3.py:124\u001b[0m, in \u001b[0;36mLayoutLMv3Processor.__call__\u001b[1;34m(self, images, text, text_pair, boxes, word_labels, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, return_tensors, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    120\u001b[0m         text \u001b[38;5;241m=\u001b[39m [text]  \u001b[38;5;66;03m# add batch dimension (as the image processor always adds a batch dimension)\u001b[39;00m\n",
      "\u001b[0;32m    121\u001b[0m     text_pair \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;32m    123\u001b[0m encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n",
      "\u001b[1;32m--> 124\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwords\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n",
      "\u001b[0;32m    125\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[0;32m    126\u001b[0m     boxes\u001b[38;5;241m=\u001b[39mboxes \u001b[38;5;28;01mif\u001b[39;00m boxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n",
      "\u001b[0;32m    127\u001b[0m     word_labels\u001b[38;5;241m=\u001b[39mword_labels,\n",
      "\u001b[0;32m    128\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n",
      "\u001b[0;32m    129\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n",
      "\u001b[0;32m    130\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n",
      "\u001b[0;32m    131\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n",
      "\u001b[0;32m    132\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n",
      "\u001b[0;32m    133\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n",
      "\u001b[0;32m    134\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n",
      "\u001b[0;32m    135\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n",
      "\u001b[0;32m    136\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n",
      "\u001b[0;32m    137\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n",
      "\u001b[0;32m    138\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n",
      "\u001b[0;32m    139\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n",
      "\u001b[0;32m    140\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n",
      "\u001b[0;32m    141\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n",
      "\u001b[0;32m    142\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n",
      "\u001b[0;32m    143\u001b[0m )\n",
      "\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# add pixel values\u001b[39;00m\n",
      "\u001b[0;32m    146\u001b[0m images \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\feature_extraction_utils.py:86\u001b[0m, in \u001b[0;36mBatchFeature.__getitem__\u001b[1;34m(self, item)\u001b[0m\n",
      "\u001b[0;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03mIf the key is a string, returns the value of the dict associated to `key` ('input_values', 'attention_mask',\u001b[39;00m\n",
      "\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03metc.).\u001b[39;00m\n",
      "\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexing with integers is not available when using Python based feature extractors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "\u001b[1;31mKeyError\u001b[0m: 'words'"
     ]
    }
   ],
   "source": [
    "# Create a test image (you would replace this with your actual image)\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Download a sample invoice image for testing\n",
    "# url = \"https://raw.githubusercontent.com/microsoft/unilm/master/layoutlmv3/examples/invoice.png\"\n",
    "# response = requests.get(url)\n",
    "image = Image.open(\"image.png\").convert('RGB')\n",
    "\n",
    "# Prepare the image for the model\n",
    "encoding = processor(\n",
    "    images=image,\n",
    "    text=None,  # No text input needed for classification\n",
    "    boxes=[[0, 0, 1000, 1000]],  # Add dummy bounding box\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Forward pass through the model\n",
    "outputs = model(**encoding)\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = outputs.logits.argmax(-1).item()\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "\n",
    "# Get the confidence scores\n",
    "confidence_scores = outputs.logits.softmax(-1)\n",
    "print(f\"Confidence scores: {confidence_scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[27], line 12\u001b[0m\n",
      "\u001b[0;32m      9\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Prepare the image for the model\u001b[39;00m\n",
      "\u001b[1;32m---> 12\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# No text input needed for classification\u001b[39;49;00m\n",
      "\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Add dummy bounding box\u001b[39;49;00m\n",
      "\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n",
      "\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n",
      "\u001b[0;32m     20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoding)\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\layoutlmv3\\processing_layoutlmv3.py:124\u001b[0m, in \u001b[0;36mLayoutLMv3Processor.__call__\u001b[1;34m(self, images, text, text_pair, boxes, word_labels, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, return_tensors, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    120\u001b[0m         text \u001b[38;5;241m=\u001b[39m [text]  \u001b[38;5;66;03m# add batch dimension (as the image processor always adds a batch dimension)\u001b[39;00m\n",
      "\u001b[0;32m    121\u001b[0m     text_pair \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;32m    123\u001b[0m encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n",
      "\u001b[1;32m--> 124\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwords\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n",
      "\u001b[0;32m    125\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[0;32m    126\u001b[0m     boxes\u001b[38;5;241m=\u001b[39mboxes \u001b[38;5;28;01mif\u001b[39;00m boxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n",
      "\u001b[0;32m    127\u001b[0m     word_labels\u001b[38;5;241m=\u001b[39mword_labels,\n",
      "\u001b[0;32m    128\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n",
      "\u001b[0;32m    129\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n",
      "\u001b[0;32m    130\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n",
      "\u001b[0;32m    131\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n",
      "\u001b[0;32m    132\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n",
      "\u001b[0;32m    133\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n",
      "\u001b[0;32m    134\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n",
      "\u001b[0;32m    135\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n",
      "\u001b[0;32m    136\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n",
      "\u001b[0;32m    137\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n",
      "\u001b[0;32m    138\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n",
      "\u001b[0;32m    139\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n",
      "\u001b[0;32m    140\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n",
      "\u001b[0;32m    141\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n",
      "\u001b[0;32m    142\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n",
      "\u001b[0;32m    143\u001b[0m )\n",
      "\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# add pixel values\u001b[39;00m\n",
      "\u001b[0;32m    146\u001b[0m images \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\feature_extraction_utils.py:86\u001b[0m, in \u001b[0;36mBatchFeature.__getitem__\u001b[1;34m(self, item)\u001b[0m\n",
      "\u001b[0;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03mIf the key is a string, returns the value of the dict associated to `key` ('input_values', 'attention_mask',\u001b[39;00m\n",
      "\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03metc.).\u001b[39;00m\n",
      "\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexing with integers is not available when using Python based feature extractors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "\u001b[1;31mKeyError\u001b[0m: 'words'"
     ]
    }
   ],
   "source": [
    "# Create a test image (you would replace this with your actual image)\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Download a sample invoice image for testing\n",
    "# url = \"https://raw.githubusercontent.com/microsoft/unilm/master/layoutlmv3/examples/invoice.png\"\n",
    "# response = requests.get(url)\n",
    "image = Image.open(\"image.png\").convert('RGB')\n",
    "\n",
    "# Prepare the image for the model\n",
    "encoding = processor(\n",
    "    images=image,\n",
    "    text=None,  # No text input needed for classification\n",
    "    boxes=[[0, 0, 1000, 1000]],  # Add dummy bounding box\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Forward pass through the model\n",
    "outputs = model(**encoding)\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = outputs.logits.argmax(-1).item()\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "\n",
    "# Get the confidence scores\n",
    "confidence_scores = outputs.logits.softmax(-1)\n",
    "print(f\"Confidence scores: {confidence_scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[27], line 12\u001b[0m\n",
      "\u001b[0;32m      9\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Prepare the image for the model\u001b[39;00m\n",
      "\u001b[1;32m---> 12\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# No text input needed for classification\u001b[39;49;00m\n",
      "\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Add dummy bounding box\u001b[39;49;00m\n",
      "\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n",
      "\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n",
      "\u001b[0;32m     20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoding)\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\layoutlmv3\\processing_layoutlmv3.py:124\u001b[0m, in \u001b[0;36mLayoutLMv3Processor.__call__\u001b[1;34m(self, images, text, text_pair, boxes, word_labels, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, return_tensors, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    120\u001b[0m         text \u001b[38;5;241m=\u001b[39m [text]  \u001b[38;5;66;03m# add batch dimension (as the image processor always adds a batch dimension)\u001b[39;00m\n",
      "\u001b[0;32m    121\u001b[0m     text_pair \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;32m    123\u001b[0m encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n",
      "\u001b[1;32m--> 124\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwords\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n",
      "\u001b[0;32m    125\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[0;32m    126\u001b[0m     boxes\u001b[38;5;241m=\u001b[39mboxes \u001b[38;5;28;01mif\u001b[39;00m boxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n",
      "\u001b[0;32m    127\u001b[0m     word_labels\u001b[38;5;241m=\u001b[39mword_labels,\n",
      "\u001b[0;32m    128\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n",
      "\u001b[0;32m    129\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n",
      "\u001b[0;32m    130\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n",
      "\u001b[0;32m    131\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n",
      "\u001b[0;32m    132\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n",
      "\u001b[0;32m    133\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n",
      "\u001b[0;32m    134\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n",
      "\u001b[0;32m    135\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n",
      "\u001b[0;32m    136\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n",
      "\u001b[0;32m    137\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n",
      "\u001b[0;32m    138\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n",
      "\u001b[0;32m    139\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n",
      "\u001b[0;32m    140\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n",
      "\u001b[0;32m    141\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n",
      "\u001b[0;32m    142\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n",
      "\u001b[0;32m    143\u001b[0m )\n",
      "\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# add pixel values\u001b[39;00m\n",
      "\u001b[0;32m    146\u001b[0m images \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\feature_extraction_utils.py:86\u001b[0m, in \u001b[0;36mBatchFeature.__getitem__\u001b[1;34m(self, item)\u001b[0m\n",
      "\u001b[0;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03mIf the key is a string, returns the value of the dict associated to `key` ('input_values', 'attention_mask',\u001b[39;00m\n",
      "\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03metc.).\u001b[39;00m\n",
      "\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexing with integers is not available when using Python based feature extractors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "\u001b[1;31mKeyError\u001b[0m: 'words'"
     ]
    }
   ],
   "source": [
    "# Create a test image (you would replace this with your actual image)\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Download a sample invoice image for testing\n",
    "# url = \"https://raw.githubusercontent.com/microsoft/unilm/master/layoutlmv3/examples/invoice.png\"\n",
    "# response = requests.get(url)\n",
    "image = Image.open(\"image.png\").convert('RGB')\n",
    "\n",
    "# Prepare the image for the model\n",
    "encoding = processor(\n",
    "    images=image,\n",
    "    text=None,  # No text input needed for classification\n",
    "    boxes=[[0, 0, 1000, 1000]],  # Add dummy bounding box\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Forward pass through the model\n",
    "outputs = model(**encoding)\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = outputs.logits.argmax(-1).item()\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "\n",
    "# Get the confidence scores\n",
    "confidence_scores = outputs.logits.softmax(-1)\n",
    "print(f\"Confidence scores: {confidence_scores}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
