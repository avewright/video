{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Fixed NumPy Compatibility Issues\n",
    "\n",
    "**IMPORTANT:** Before running this notebook, restart your kernel to ensure the NumPy downgrade takes effect:\n",
    "- In Jupyter: Kernel ‚Üí Restart Kernel\n",
    "- In VS Code: Restart the Python interpreter\n",
    "\n",
    "## Changes Made:\n",
    "1. **Fixed NumPy compatibility**: Downgraded NumPy from 2.2.6 to 1.26.4 to be compatible with PyTorch 2.1.0\n",
    "2. **Added error handling**: Better debugging information throughout the notebook\n",
    "3. **Improved model loading**: Added progress messages and trust_remote_code parameter\n",
    "4. **Enhanced inference function**: Better error handling and device management\n",
    "\n",
    "## Next Steps:\n",
    "Run all cells in order after restarting the kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify NumPy compatibility\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(\"üîç Checking system compatibility...\")\n",
    "print(f\"‚úÖ NumPy version: {np.__version__}\")\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Test NumPy-PyTorch compatibility\n",
    "try:\n",
    "    # This was the operation that was failing before\n",
    "    test_array = np.array([1, 2, 3])\n",
    "    test_tensor = torch.from_numpy(test_array)\n",
    "    print(\"‚úÖ NumPy-PyTorch compatibility: WORKING\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå NumPy-PyTorch compatibility: FAILED - {e}\")\n",
    "\n",
    "print(\"\\nüöÄ Ready to proceed with model loading!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.33.2)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.3.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.53.0)\n",
      "Requirement already satisfied: qwen-vl-utils in /usr/local/lib/python3.10/dist-packages (0.0.11)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.6.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (1.1.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (from qwen-vl-utils) (14.4.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.0+cu118)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.8.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade huggingface_hub pillow transformers qwen-vl-utils datasets peft "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load token from environment variable (safer than hardcoding)\n",
    "import os\n",
    "kahua_token = os.getenv('HUGGINGFACE_TOKEN', 'your-token-here')\n",
    "\n",
    "# If no environment variable is set, you'll need to set it\n",
    "if kahua_token == 'your-token-here':\n",
    "    print(\"‚ö†Ô∏è  Please set your HUGGINGFACE_TOKEN environment variable\")\n",
    "    print(\"   export HUGGINGFACE_TOKEN='your-actual-token'\")\n",
    "    # For now, you can uncomment and set your token here temporarily:\n",
    "    # kahua_token = 'your-actual-token-here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_hub.login(kahua_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.0+cu118\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "import datasets\n",
    "from PIL import Image\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen2.5-VL-3B-Instruct model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e055afaa5c5749f98a44bca53a00fd98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Loading processor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Qwen2.5-VL-3B-Instruct model...\")\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# default processer\n",
    "print(\"Loading processor...\")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\", trust_remote_code=True)\n",
    "print(\"Processor loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PEFT config from kahua-ml/invoice1...\n",
      "PEFT config loaded successfully!\n",
      "Enabling input gradients...\n",
      "Loading PEFT model...\n",
      "PEFT model loaded successfully!\n",
      "Model device: cuda:0\n",
      "Model dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load the config\n",
    "peft_model_id = \"kahua-ml/invoice1\"\n",
    "print(f\"Loading PEFT config from {peft_model_id}...\")\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
    "print(\"PEFT config loaded successfully!\")\n",
    "\n",
    "print(\"Enabling input gradients...\")\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# Attach the PEFT model\n",
    "print(\"Loading PEFT model...\")\n",
    "peft_model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "print(\"PEFT model loaded successfully!\")\n",
    "\n",
    "print(f\"Model device: {next(peft_model.parameters()).device}\")\n",
    "print(f\"Model dtype: {next(peft_model.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(image, model, processor):\n",
    "    \"\"\"Run inference on the image.\"\"\"\n",
    "    print(\"üöÄ Starting inference...\")\n",
    "    \n",
    "    # Use the EXACT training prompt for better results\n",
    "    query = \"\"\"You are an expert at extracting structured data from receipts and invoices. \n",
    "Analyze the image and return in JSON format all metadata seen including company details, items, prices, totals, and dates.\n",
    "\n",
    "Expected JSON format:\n",
    "{\n",
    "  \"company\": \"Company Name\",\n",
    "  \"address\": \"Full Address\", \n",
    "  \"date\": \"YYYY-MM-DD\",\n",
    "  \"total\": \"XX.XX\",\n",
    "  \"tax\": \"XX.XX\",\n",
    "  \"items\": [\n",
    "    {\n",
    "      \"description\": \"Item description\",\n",
    "      \"quantity\": \"X\",\n",
    "      \"price\": \"XX.XX\",\n",
    "      \"total\": \"XX.XX\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "JSON Output:\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": query}\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(\"üìù Applying chat template...\")\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    print(\"üñºÔ∏è Processing vision info...\")\n",
    "    vision_info = process_vision_info(messages)\n",
    "    image_inputs = vision_info[0] if len(vision_info) > 0 else None\n",
    "    video_inputs = vision_info[1] if len(vision_info) > 1 else None\n",
    "    \n",
    "    print(f\"üìä Image inputs type: {type(image_inputs)}\")\n",
    "    print(f\"üìä Number of images: {len(image_inputs) if image_inputs else 0}\")\n",
    "    \n",
    "    print(\"‚öôÔ∏è Processing inputs with alternative approach...\")\n",
    "    # Try different approaches for processing\n",
    "    try:\n",
    "        # First approach: Standard processing\n",
    "        inputs = processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        print(\"‚úÖ Standard processing successful!\")\n",
    "    except Exception as e1:\n",
    "        print(f\"‚ö†Ô∏è Standard processing failed: {e1}\")\n",
    "        try:\n",
    "            # Second approach: Process without explicit padding\n",
    "            inputs = processor(\n",
    "                text=[text],\n",
    "                images=image_inputs,\n",
    "                videos=video_inputs,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            print(\"‚úÖ Alternative processing successful!\")\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Alternative processing also failed: {e2}\")\n",
    "            # Third approach: Manual processing\n",
    "            print(\"üîß Trying manual processing...\")\n",
    "            inputs = processor.tokenizer(\n",
    "                text, return_tensors=\"pt\", padding=True, truncation=True\n",
    "            )\n",
    "            # Process images separately\n",
    "            if image_inputs:\n",
    "                image_features = processor.image_processor(\n",
    "                    images=image_inputs, return_tensors=\"pt\"\n",
    "                )\n",
    "                inputs.update(image_features)\n",
    "            print(\"‚úÖ Manual processing successful!\")\n",
    "\n",
    "    print(\"üîÑ Moving inputs to device...\")\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    print(\"üß† Generating response...\")\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=512, \n",
    "            do_sample=False,\n",
    "            temperature=1.0,\n",
    "            top_p=1.0\n",
    "        )\n",
    "    \n",
    "    print(\"üìñ Decoding response...\")\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Inference completed!\")\n",
    "    return output_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading image: /root/video/rryalsty.png\n",
      "Image size: (938, 548)\n",
      "Starting inference...\n",
      "Applying chat template...\n",
      "Processing vision info...\n",
      "Image inputs type: <class 'list'>\n",
      "Number of images: 1\n",
      "Processing inputs...\n",
      "Inputs processed successfully!\n",
      "Moving inputs to device...\n",
      "Generating response...\n",
      "Decoding response...\n",
      "Inference completed!\n",
      "\n",
      "==================================================\n",
      "INFERENCE RESULT:\n",
      "==================================================\n",
      "{\n",
      "  \"company\": \"IRONHORSE\",\n",
      "  \"product\": \"INDUSTRIAL DUTY FRACTIONAL MOTOR\",\n",
      "  \"model\": \"MTRJ-P33-3BD36J\",\n",
      "  \"frame\": \"56J\",\n",
      "  \"frequency\": {\n",
      "    \"60\": \"\",\n",
      "    \"50\": \"\"\n",
      "  },\n",
      "  \"horsepower\": {\n",
      "    \"1/3\": \"\",\n",
      "    \"1/4\": \"\"\n",
      "  },\n",
      "  \"phase\": \"3\",\n",
      "  \"rpm\": {\n",
      "    \"3450\": \"\",\n",
      "    \"2850\": \"\"\n",
      "  },\n",
      "  \"duty\": \"CONT\",\n",
      "  \"voltage\": {\n",
      "    \"230/460\": \"\",\n",
      "    \"190/380\": \"\"\n",
      "  },\n",
      "  \"amps\": {\n",
      "    \"1.3/0.65\": \"\",\n",
      "    \"1.2/0.6\": \"\"\n",
      "  },\n",
      "  \"insulation\": \"F\",\n",
      "  \"ip rating\": \"IP43\",\n",
      "  \"s.f\": \"1.15\",\n",
      "  \"sfa\": {\n",
      "    \"1.5/0.75\": \"\",\n",
      "    \"1.2/0.6\": \"\"\n",
      "  },\n",
      "  \"lb/wt\": \"18\",\n",
      "  \"max. ambient temperature\": \"40¬∞C\",\n",
      "  \"date code\": \"01/2022\",\n",
      "  \"serial number\": \"2022010008\",\n",
      "  \"acceptance for field wiring\": \"NONE\",\n",
      "  \"website\": \"WWW.AUTOMATIONDIRECT.COM\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Check if image file exists\n",
    "image_path = r\"/root/video/rryalsty.png\"\n",
    "if not os.path.exists(image_path):\n",
    "    print(f\"Error: Image file '{image_path}' not found!\")\n",
    "    print(\"Available files in current directory:\")\n",
    "    print([f for f in os.listdir(\".\") if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "else:\n",
    "    print(f\"Loading image: {image_path}\")\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    print(f\"Image size: {image.size}\")\n",
    "    \n",
    "    try:\n",
    "        result = infer(image, peft_model, processor)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"INFERENCE RESULT:\")\n",
    "        print(\"=\"*50)\n",
    "        print(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
